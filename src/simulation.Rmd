---
title: "Simulation"
author: "Julian Frattini"
date: '2024-03-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(brms)
library(broom.mixed)
library(janitor)
library(tidybayes)
library(marginaleffects)

set.seed(42)
```

In this notebook, we perform a simulation of our assumptions to demonstrate the capability of a Bayesian model to recover the strength of effects causing the observed data.

## Data synthesis

Firstly, we synthesize a data set in which we know the effect of certain variables on our response variable.
We determine $n=50$ participants and assume a $2\times2$ factorial crossover design.
This means, the synthesized data set simulates an experiment with 2 levels of the main factor (i.e., baseline and treatment) administered to every participant in 2 periods.

```{r constants}
# number of hypothesized participants in the experiment
n.participants <- 50

# number of periods
n.periods <- 2
periods <- data.frame(
  period = seq(1, n.periods)
)
```

Next, we determine the strength of the effect that each variable has on the response variable.
To avoid researcher bias, we randomly sample these effect strength values from a random distribution $\mathcal{N}(0, 1)$.

```{r effects}
effects <- rnorm(6, 0, 1)

e.baseline <- effects[1]
e.treatment <- effects[2]
e.skill <- effects[3]
e.crossover.AB <- effects[4] # baseline -> treatment
e.crossover.BA <- effects[5] # treatment -> baseline
e.learning <- effects[6]
```

```{r effects-output}
d.effects <- data.frame(
  Effect = c("Baseline (Intercept)", "Treatment", "Individual skill", "Crossover Effect (AB)", "Crossover Effect (BA)", "Learning Effect"),
  Strength = c(e.baseline, e.treatment, e.skill, e.crossover.AB, e.crossover.BA, e.learning)
) 

d.effects %>% knitr::kable("simple")
```

We construct a data frame of participants, consisting of their ID, a randomly assigned skill value (also $\mathcal{N}(0, 1)$) representing their inherent capability, and an assignment to a experimental group (either AB (`group=1`), which receives the treatment in period 2, or group BA (`group=2`), which receives the treatment in period 1).
The two variables of participants that influence the response variable (group and skill) are not correlated.

```{r data-participants}
# generate IDs for each participant
participants.id <- seq(1, n.participants)

# randomly assign a "skill" value to each participant
participants.skill <- rnorm(n.participants, 0, 1)

# assemple a data frame of participants
participants <- data.frame(
  PID = participants.id,
  skill = participants.skill,
  group = rep(c(1, 2), n.participants/2) # assignment of participants to groups
)
```

Finally, we generate synthetic values of the `response` variable according to our causal model.
This means, the response variable is calculated via random draws from a normal distribution where the mean $\mu$ is calculated via all causal variables according to their respective effect strength.
We set the standard deviation $\sigma$ to 0.3, which introduces slight variation around the calculated values.

```{r data-response}
observations <- participants %>% 
  cross_join(periods) %>% 
  mutate(treatment = (group!=period)) %>% 
  mutate(
    response = rnorm(n.participants*n.periods, 
                     e.baseline + e.treatment*treatment + e.learning*(period-1) + e.skill*skill + e.crossover.AB*(period-1)*(abs(group-2)) + e.crossover.BA*(period-1)*(group-1),
                     0.3)
  )
```

## Data Analysis

Now, we analyze the data with our proposed approach.

### Formula and Priors

We define a linear model that determines the `response` variable in relation to the causally related variables.
The formula consists of the following terms:

| Term | Variable | Meaning |
|---|---|---|
| `1` | Intercept | Fixed effect (population-wise) on the response variable |
| `treatment` | Treatment | Effect of the treatment |
| `period` | Learning | Effect of repeating the experimental task |
| `treatment*period` | Carryover | Effect of administering the treatment in period 1 on period 2 |
| `(1|PID)` | Skill | Random effect (group-wise) of the capability of each participant |

The `treatment` and `period` term are added automatically via the `treatment*period` interaction.

```{r formula}
f1 <- response ~ 1 + treatment*period + (1|PID)

get_prior(f1, data=observations, family=gaussian)
```

We select uninformative priors, which are mainly normal distributions centered around the mean 0 ($\mathcal{N}(0, 1)$).
This ensures that we introduce no bias to the resulting posterior predictions.

```{r priors}
priors1 <- c(
  prior(normal(0, 1), class=Intercept),
  prior(normal(0, 1), class=b)
)
```

We sample draws directly from the priors (i.e., sample without the model updating its parameter distributions based on the observed data) to evaluate the feasibility of the priors. 

```{r model-priors}
m1.prior <-
  brm(data = observations, family = gaussian, f1, prior = priors1,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4, sample_prior="only"
  )
```

With the samples from the prior distributions, we conduct a prior predictive check to ensure that the priors are appropriate, i.e., the actual observations are realistic given the prior knowledge of the model.

```{r prior-predictive-check}
ndraws <- 100

brms::pp_check(m1.prior, ndraws=ndraws)
```

The distribution of draws encompasses the actually observed data, meaning that the actual observations lie within the realm of belief defined by the priors. We accept our priors as feasible.

### Model Training

With the appropriateness of priors confirmed, we train the model on the actual data.

```{r model}
m1 <-
  brm(data = observations, family = gaussian, f1, prior = priors1,
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 4
  )
```

Again, we perform a posterior predictive check to ensure that the predictions of the model are feasible.

```{r posterior-predictive-check}
brms::pp_check(m1, ndraws=ndraws)
```

### Model Evaluation

Finally, we evaluate the resulting model.

```{r summary}
summary(m1)
```

#### Fixed Effects

First, we investigate the coefficients of fixed effects to see how well they align with the effect strength that we determined before.

```{r fixed-effects}
# obtain the fixed effects in the model
coefs.fixed <- data.frame(fixef(m1, summary = TRUE))

# make the name of the factor (which was previously the index of the data frame) an explicit column
coefs.fixed <- cbind(factor = rownames(coefs.fixed), coefs.fixed)

# add a numeric column to the updated data frame
rownames(coefs.fixed) <- 1:nrow(coefs.fixed)
```

Then we add a column to our data frame containing the actual effects defined by us where the names of the factors align with the output from the model.

```{r effects-df-rename}
d.effects.coefs <- cbind(factor = c(
  'Intercept', 'treatmentTRUE', 'PID', 'treatmentTRUE:period', 'treatmentFALSE:period', 'period'
  ), d.effects) %>% 
  filter(factor %in% coefs.fixed$factor)
```

Now, we can visualize the mean and confidence interval of the calculated coefficient distributions (black errorbars) and compare them to the actual effect defined by us (red dot).

```{r fixed-effect-distributions}
coefs.fixed %>% ggplot(aes(y=factor)) +
  geom_point(aes(x=Estimate)) +
  geom_errorbar(aes(xmin=Q2.5, xmax=Q97.5), width=0.3) +
  geom_point(aes(x=Strength), data=d.effects.coefs, size=3, color="red") +
  geom_vline(xintercept = 0, linetype="dashed", color="grey")
```

#### Random Effects

The random effect of the participant ID `(1|PID)` should model the `skill` value of each participant.
We would expect that the intercept of the random effect equals the skill value times the skill effect.

```{r pid-summary}
PID.impact <- data.frame(ranef(m1)[1]$PID)
```

A scatter plot visualizes the distribution of $participants.skill \times e.skill$ against the mean intercept of each participant.

```{r pid-skill-scatter}
data.frame(
  skill = participants.skill * e.skill,
  PID.intercept = PID.impact$Estimate.Intercept
) %>% 
  ggplot(aes(x=skill, y=PID.intercept)) +
  geom_point() +
  geom_abline(intercept = 0, slope=1)
```

We can quantify this correlation by calculating a correlation coefficient.
Firstly, we determine whether the two distributions are normally distributed.

```{r pid-skill-normal}
shapiro.test(participants.skill)
shapiro.test(PID.impact$Estimate.Intercept)
```

The Shapiro-Wilk tests suggests that both distributions are normal.
This is as expected, since $participants.skill \sim \mathcal{N}(0, 1)$ and the intercepts per PID should resemble it.
Consequently, we can calculate the Pearson correlation coefficient.

```{r pid-skill-correlation}
cor.test(x=participants$skill, y=PID.impact$Estimate.Intercept, method="pearson")
```

This confirms that the inclusion of a participant-level random effect `(1|PID)` adequately models the individual skill of a participant and factors it out from the relationship of interest.
